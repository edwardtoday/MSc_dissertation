%!TEX root = featurematch.tex
\subsection{Dimension Reduction}
\label{ssec:methodology:lda}

%TODO find the citation
LDA is a state-of-the-art dimensionality reduction technique widely used in classification problems. The objective is to find the optimal projection which simultaneously minimizes the within-class distance and maximizes the between-class distance, thus achieving maximum discrimination (Here, the “class” is used to denote the identity of the subjects, e.g. the samples collected from one palm are regarded as one class). However, the traditional LDA requires the within-class scatter matrix to be nonsingular, which means the sample size should be large enough compared with its dimension, but is not always possible. In this paper, we therefore adopt the orthogonal LDA (OLDA) proposed in ~\cite{Ye:2005uu}, where the vectors of the optimal projection are calculated using the training database and the optimal projecting vectors are orthogonal to each other.

Suppose the 3D ROI has been divided to N levels and that M radial lines are used to represent the level contours. We can list the features as a column vector,
\begin{equation}
F=\{MD,A^1,A^2,\dots,A^N,R^1,R^2,\dots,R^{N\times M}\}
\end{equation}
with $1+N+N\times M$ rows. Given a training database which has $n$ samples and $k$ classes as $X=[X_1,X_2,\dots,X_k]$, where $X_i \in \mathbb{R}^{(1+N+N\times M)\times n}, i=1,2,\dots,k$ and $n=\sum \limits_{i=1}^k n_i$ , adopting OLDA ~\cite{Ye:2005uu} the optimal projection $W$ can be calculated as follows.

First, the within-class scatter matrix $S_w$, the between-class scatter matrix $S_b$ and total scatter matrix $S_t$ can be expressed as

\begin{equation}
S_w=H_w H_w^T, S_b=H_b H_b^T, S_t=H_t H_t^T
\end{equation}

where

\begin{equation}
H_w=\frac{1}{\sqrt{n}}\left[X_1 - m_1 \cdot e_1^T, \dots, X_k - m_k \cdot e_k^T \right]
\end{equation}

\begin{equation}
H_b=\frac{1}{\sqrt{n}}\left[\sqrt{n_1}(m_1-M),\dots,\sqrt{n_k}(m_k-m) \right]
\end{equation}

\begin{equation}
H_t=\frac{1}{\sqrt{n}}(X-m\cdot e^T)
\end{equation}

                                         (9)
where $m_i$ is the centroid of the $i$th class $X_i$, $m$ is the centroid of all the training samples $X$, $e_i=[1,1,\dots,1]^T \in \mathbb{R}^n, i=1,2,\dots,k$ and $e=[1,1,\dots,1]^T \in \mathbb{R}^n$.

After calculating $H_w$, $H_b$ and $H_t$, the reduced Singular Value Decomposition (SVD) is applied to $H_t$.

\begin{equation}
H_t \xrightarrow{\text{Reduced SVD}} U_r \Sigma_r V_r^T
\end{equation}

Denote $B=\Sigma_r^{-1} U_r^T H_b$ and compute the SVD of B.

\begin{equation}
B \xrightarrow{\text{SVD}} U_B \Sigma_B V_B^T
\end{equation}

Let

\begin{equation}
D=U_r\Sigma_r^{-1}U_B
\end{equation}

\begin{equation}
q=rank(B)
\end{equation}

and denote $D_q$ the first $q$ columns of the matrix D. Then, compute the QR decomposition of $D_q$.

\begin{equation}
D_q \xrightarrow{\text{QR decomposition}} QR
\end{equation}

where $Q$ is the desired orthogonal matrix and optimal projection, i.e. $W=Q$.

After getting the optimal projection $W$, we can map the $1+N+N\times M$ dimensional vector $F$ to a lower dimensional space

\begin{equation}
\tilde{F}=W^T F
\end{equation}
                                             (15)
where $\tilde{F}={f_1,f_2,\dots,f_{\Gamma}}$ is a $\Gamma$ dimensional vector with $\Gamma<1+N+N\times M$.
